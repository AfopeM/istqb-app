{
  "questions": [
    {
      "id": "q001",
      "chapterSection": "2.1",
      "questionText": "Which statement best describes the difference between sequential and iterative SDLC models in terms of dynamic testing?",
      "options": [
        "Sequential models focus mostly on system testing, while iterative models focus on unit testing",
        "Iterative models eliminate the need for static testing, while sequential models rely heavily on it",
        "Sequential models introduce dynamic testing mainly after coding is complete, while iterative models introduce it progressively during development",
        "Iterative models always require automated testing, while sequential models depend mostly on manual testing"
      ],
      "correctAnswerIndex": 2,
      "explanation": "In sequential models, executable code is created late in the cycle, so dynamic testing happens after most development is finished. Iterative models deliver working software earlier in increments, which enables dynamic testing throughout development."
    },
    {
      "id": "q002",
      "chapterSection": "2.1",
      "questionText": "What is the main purpose of a Software Development Lifecycle (SDLC) model?",
      "options": [
        "To define the programming languages and tools used for implementation",
        "To provide a structured view of how development phases and activities relate over time",
        "To specify the detailed project timeline and budget estimates",
        "To determine team size, structure, and responsibilities"
      ],
      "correctAnswerIndex": 1,
      "explanation": "An SDLC model is an abstract representation of how development phases and activities are organized logically and chronologically, not about tools, budgets, or team structures."
    },
    {
      "id": "q003",
      "chapterSection": "2.1",
      "questionText": "Which of the following is an example of an incremental development model?",
      "options": [
        "Waterfall model, because it moves strictly through phases in order",
        "V-model, because it combines sequential development with planned test phases",
        "Unified Process, because it delivers functionality in increments across iterations",
        "Spiral model, because it emphasizes repeated planning and risk analysis cycles"
      ],
      "correctAnswerIndex": 2,
      "explanation": "The Unified Process is an incremental model since it delivers usable increments in each iteration. Waterfall and V-model are sequential, while Spiral is iterative but not incremental."
    },
    {
      "id": "q004",
      "chapterSection": "2.1.1",
      "questionText": "In Agile development, which testing approach is emphasized because requirements may change frequently?",
      "options": [
        "Detailed upfront test analysis with comprehensive documentation",
        "Formal verification techniques with strict traceability",
        "Exploratory and experience-based techniques supported by lightweight documentation",
        "Automated regression testing only, since manual testing is considered ineffective"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Agile favors lightweight documentation and flexibility. Manual testing often uses exploratory or experience-based approaches, supported by automation, without heavy upfront planning."
    },
    {
      "id": "q005",
      "chapterSection": "2.1.1",
      "questionText": "Which aspect is least influenced by the choice of SDLC model?",
      "options": [
        "The programming language used in implementation",
        "The timing and scope of test activities",
        "The extent to which test automation is used",
        "The tester’s role and responsibilities"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Programming language selection is a technical decision, not directly related to the SDLC model. Models affect scope, timing, automation, and tester responsibilities."
    },
    {
      "id": "q006",
      "chapterSection": "2.1.1",
      "questionText": "What is a defining feature of testing in iterative and incremental development?",
      "options": [
        "Testing is postponed until all increments are integrated",
        "Each iteration may involve static and dynamic testing at multiple levels",
        "Manual testing is avoided since iterative models rely exclusively on automation",
        "Test documentation is fixed at project start and reused for all iterations"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Iterative and incremental models assume each iteration produces a working increment, enabling both static and dynamic testing at all test levels throughout development."
    },
    {
      "id": "q007",
      "chapterSection": "2.1.1",
      "questionText": "Why do iterative and incremental models demand more regression testing?",
      "options": [
        "Because they rely on more complex programming languages",
        "Because the development cycle is significantly longer",
        "Because they generate more formal documentation",
        "Because frequent increments require ensuring new changes do not break existing functionality"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Frequent increments in iterative and incremental models make regression testing critical to ensure stability as new functionality is delivered."
    },
    {
      "id": "q008",
      "chapterSection": "2.1.2",
      "questionText": "When should test analysis and design ideally begin for a given test level?",
      "options": [
        "Once the related development deliverables are drafted, even before coding ends",
        "Only after the corresponding development phase is completely finished",
        "When executable code for that level is available",
        "At the end of each iteration during project closure"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Good practice is to begin test analysis and design as soon as related work products are drafted, supporting early testing and defect prevention."
    },
    {
      "id": "q009",
      "chapterSection": "2.1.2",
      "questionText": "How should development and test activities relate to one another in good testing practice?",
      "options": [
        "Test activities should be performed only after development is complete",
        "Test activities should run in isolation, independent of development",
        "Test activities should concentrate mainly on the final deliverable",
        "Test activities should correspond to each development activity to ensure ongoing quality"
      ],
      "correctAnswerIndex": 3,
      "explanation": "For every development activity, there should be a matching test activity. This ensures quality is built in throughout the lifecycle, not just at the end."
    },
    {
      "id": "q010",
      "chapterSection": "2.1.2",
      "questionText": "Why should testers review work products as soon as draft versions are available?",
      "options": [
        "To take over the developer’s workload in identifying coding errors",
        "To complete detailed test scripts before the coding phase begins",
        "To detect issues early, reducing the cost and effort of fixing defects later",
        "To remove the need for structured testing phases later in the project"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Early reviews support the shift-left principle, allowing defects to be identified and fixed when they are cheaper and easier to correct."
    },
    {
      "id": "q011",
      "chapterSection": "2.1.2",
      "questionText": "Why are different test levels defined with specific objectives?",
      "options": [
        "To align testing activities with the organizational hierarchy of teams",
        "To ensure compliance with external standards and regulatory frameworks",
        "To give testers opportunities to specialize in particular areas of testing",
        "To achieve thorough coverage by focusing on different objectives while avoiding duplication"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Different test levels have distinct objectives, which ensures testing is comprehensive across the lifecycle without unnecessary overlap or redundancy."
    },
    {
      "id": "q012",
      "chapterSection": "2.1.3",
      "questionText": "What distinguishes Behavior-Driven Development (BDD) from Test-Driven Development (TDD)?",
      "options": [
        "BDD focuses mainly on low-level unit tests, while TDD emphasizes business-facing acceptance tests",
        "BDD uses scenarios written in Given/When/Then format to be understandable by both technical and non-technical stakeholders",
        "BDD involves fewer refactoring cycles than TDD since behavior is defined upfront",
        "BDD is primarily intended for sequential lifecycle models, while TDD is mostly applied in agile"
      ],
      "correctAnswerIndex": 1,
      "explanation": "BDD expresses system behavior in natural language scenarios using Given/When/Then, making them readable by stakeholders. TDD, while also test-first, does not rely on stakeholder-friendly syntax."
    },
    {
      "id": "q013",
      "chapterSection": "2.1.3",
      "questionText": "When are acceptance tests typically created in Acceptance Test-Driven Development (ATDD)?",
      "options": [
        "After the software feature is fully implemented",
        "At the same time as the feature is being coded",
        "Before the feature is developed, based on agreed acceptance criteria",
        "During final integration testing near project completion"
      ],
      "correctAnswerIndex": 2,
      "explanation": "In ATDD, acceptance tests are defined upfront from acceptance criteria, guiding development so that the feature is built to satisfy them from the start."
    },
    {
      "id": "q014",
      "chapterSection": "2.1.3",
      "questionText": "What do TDD, ATDD, and BDD approaches have in common?",
      "options": [
        "They all require highly detailed documentation to be written before development",
        "They all depend exclusively on automated testing to be effective",
        "They all work only with object-oriented programming languages and frameworks",
        "They all apply the shift-left principle by defining tests before writing the implementation code"
      ],
      "correctAnswerIndex": 3,
      "explanation": "TDD, ATDD, and BDD all emphasize early testing by creating tests before writing code, which supports a shift-left approach."
    },
    {
      "id": "q015",
      "chapterSection": "2.1.3",
      "questionText": "What is the typical workflow followed in Test-Driven Development (TDD)?",
      "options": [
        "Prepare detailed design documents, then create tests, then implement code",
        "Write a test first, write code to make the test pass, then refactor both code and tests",
        "Develop code first, then write tests afterward, followed by debugging and fixes",
        "Write test cases and code simultaneously, then validate them during integration"
      ],
      "correctAnswerIndex": 1,
      "explanation": "TDD follows a test-first cycle: create a failing test, implement code to pass it, and then refactor code and tests as needed."
    },
    {
      "id": "q016",
      "chapterSection": "2.1.3",
      "questionText": "What happens to the tests created in TDD, ATDD, and BDD after the initial development is complete?",
      "options": [
        "They are discarded once the software is working as expected",
        "They are stored only as project documentation but not executed again",
        "They are maintained as part of the automated regression test suite for future changes",
        "They are kept temporarily and removed if they no longer reveal defects"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Tests in TDD, ATDD, and BDD typically remain in the automated test suite, providing ongoing regression protection and supporting future maintenance."
    },
    {
      "id": "q017",
      "chapterSection": "2.1.4",
      "questionText": "What is the primary organizational aim of adopting DevOps?",
      "options": [
        "To merge development and operations into a single team with no separation of roles",
        "To fully automate all aspects of building, testing, and deployment",
        "To reduce development and maintenance costs through efficiency gains",
        "To promote collaboration and shared responsibility between development and operations for common objectives"
      ],
      "correctAnswerIndex": 3,
      "explanation": "DevOps seeks to foster a culture of collaboration and shared goals between development and operations, not necessarily to eliminate roles or only focus on cost-cutting."
    },
    {
      "id": "q018",
      "chapterSection": "2.1.4",
      "questionText": "From a testing perspective, how does Continuous Integration (CI) in DevOps encourage developers to submit higher-quality code?",
      "options": [
        "By reducing the need for manual testing through continuous delivery pipelines",
        "By relying on automated regression tests to catch most defects after deployment",
        "By enforcing component tests and static analysis checks when code is submitted",
        "By ensuring stable test environments are automatically provisioned for developers"
      ],
      "correctAnswerIndex": 2,
      "explanation": "CI encourages developers to integrate frequently with tests and static analysis, ensuring issues are detected early before code is merged."
    },
    {
      "id": "q019",
      "chapterSection": "2.1.4",
      "questionText": "Which statement about manual testing in a DevOps context is most accurate?",
      "options": [
        "Mature DevOps pipelines eliminate the need for manual testing altogether",
        "Manual testing is only needed when DevOps is first being set up",
        "Manual testing remains valuable, particularly for usability and exploratory evaluation",
        "Manual testing should be restricted exclusively to non-functional performance checks"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Even in highly automated DevOps pipelines, manual testing is necessary for user-centric evaluations such as usability and exploratory testing, which automation cannot replace."
    },
    {
      "id": "q020",
      "chapterSection": "2.1.4",
      "questionText": "What is a key challenge of implementing DevOps from a testing standpoint?",
      "options": [
        "All manual testing activities must be eliminated to align with automation goals",
        "Test automation requires considerable resources to implement and maintain effectively",
        "Non-functional testing is often deprioritized and not needed in DevOps environments",
        "Extensive manual documentation must be produced before automation can begin"
      ],
      "correctAnswerIndex": 1,
      "explanation": "A major challenge in DevOps is that while automation is essential, it requires ongoing investment in infrastructure, tools, and skilled people to establish and maintain."
    },
    {
      "id": "q021",
      "chapterSection": "2.1.4",
      "questionText": "What does DevOps primarily emphasize to achieve faster, higher-quality software delivery?",
      "options": [
        "Strict handoffs between development and operations teams",
        "Heavy reliance on formal sign-offs before each release",
        "Collaborative teams with shared responsibility, rapid feedback, and practices like CI/CD",
        "Longer development cycles to ensure extensive documentation and approval"
      ],
      "correctAnswerIndex": 2,
      "explanation": "DevOps emphasizes collaboration, shared responsibility, rapid feedback, and technical practices such as CI/CD to accelerate and improve software delivery."
    },
    {
      "id": "q022",
      "chapterSection": "2.1.5",
      "questionText": "Which of the following best illustrates the shift-left testing approach?",
      "options": [
        "Automating regression testing only after deployment",
        "Designing tests before writing code and running them during development",
        "Performing usability testing after final system integration",
        "Conducting security testing exclusively during the acceptance phase"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Shift-left testing means moving testing earlier in the lifecycle, such as creating tests before code is written and running them continuously during development."
    },
    {
      "id": "q023",
      "chapterSection": "2.1.5",
      "questionText": "What is a key factor in successfully applying the shift-left approach?",
      "options": [
        "Strong stakeholder buy-in and support for earlier testing activities",
        "Testing only through full automation from the very beginning",
        "Maintaining strict separation between developers and testers",
        "Reducing documentation to accelerate coding speed"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Stakeholder support is crucial for shift-left adoption, as it often requires cultural and process changes that impact the entire organization."
    },
    {
      "id": "q024",
      "chapterSection": "2.1.5",
      "questionText": "Which statement most accurately describes the shift-left testing approach?",
      "options": [
        "It eliminates the need for testing later in the lifecycle",
        "It brings testing activities earlier but does not replace later testing phases",
        "It applies only to automated functional testing",
        "It requires all possible test cases to be defined before any coding starts"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Shift-left means testing starts earlier but does not eliminate the need for later test phases such as system or acceptance testing."
    },
    {
      "id": "q025",
      "chapterSection": "2.1.5",
      "questionText": "Which of the following is NOT considered a good practice for supporting shift-left testing?",
      "options": [
        "Reviewing requirements with a testing perspective",
        "Using CI/CD pipelines with rapid feedback loops",
        "Assigning more testers per project to catch defects faster",
        "Performing static code analysis before executing dynamic tests"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Shift-left is achieved through earlier involvement, automation, static analysis, and continuous feedback, not simply by adding more testers."
    },
    {
      "id": "q026",
      "chapterSection": "2.1.5",
      "questionText": "Why is performing non-functional testing at the component level considered shift-left?",
      "options": [
        "Because component-level testing is faster to execute than system testing",
        "Because it avoids the need for specialized testers later in the cycle",
        "Because non-functional issues can be addressed earlier when they are cheaper to fix",
        "Because non-functional testing is often delayed until full systems are available"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Non-functional testing usually happens late when full systems exist, so moving it to the component level is an example of shift-left."
    },
    {
      "id": "q027",
      "chapterSection": "2.1.5",
      "questionText": "What is a common trade-off when adopting shift-left testing?",
      "options": [
        "Overall product quality may decrease",
        "It may require extra effort, training, and costs earlier, with savings realized later",
        "It creates dependency on external vendors for specialized tools",
        "It always lengthens the overall development timeline"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Shift-left usually demands upfront investment in training, tools, and effort, but yields long-term cost and quality benefits."
    },
    {
      "id": "q028",
      "chapterSection": "2.1.6",
      "questionText": "What is the main purpose of retrospectives in testing and development?",
      "options": [
        "To assign responsibility for production defects",
        "To capture lessons learned for improving future iterations",
        "To determine individual salary increases",
        "To finalize release schedules and deadlines"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Retrospectives focus on process improvement by identifying successes, failures, and opportunities for better practices in future work."
    },
    {
      "id": "q029",
      "chapterSection": "2.1.6",
      "questionText": "What should be documented as an output of a retrospective?",
      "options": [
        "Employee performance reviews and promotions",
        "Process improvement insights recorded in the test completion report",
        "Detailed architecture diagrams for the next release",
        "Budget approvals for upcoming testing tools"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Retrospective results should be formally documented, usually in the test completion report, to guide continuous improvement."
    },
    {
      "id": "q030",
      "chapterSection": "2.1.6",
      "questionText": "Who should typically participate in retrospectives?",
      "options": [
        "Only testers and QA managers",
        "Only developers from the project team",
        "A cross-functional group including testers, developers, product owners, and business analysts",
        "Only executives and senior managers"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Effective retrospectives involve cross-functional teams, not just testers, to ensure all perspectives are considered in process improvements."
    },
    {
      "id": "q030",
      "chapterSection": "2.1.6",
      "questionText": "According to the syllabus, who should take part in retrospectives?",
      "options": [
        "Only testers and test managers",
        "Only developers and coding team members",
        "Testers, developers, architects, product owners, business analysts, and other stakeholders",
        "Only senior managers and project sponsors"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Retrospectives should involve a cross-functional group, not only testers but also developers, architects, product owners, business analysts, and other stakeholders, to ensure well-rounded improvement discussions."
    },
    {
      "id": "q031",
      "chapterSection": "2.1.6",
      "questionText": "What is a common benefit of retrospectives for testing?",
      "options": [
        "Lower tool licensing costs through optimization",
        "Improved effectiveness and efficiency by refining testing processes",
        "Complete removal of manual testing activities",
        "Full standardization of testing methods across all projects"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Retrospectives can improve testing effectiveness and efficiency through process refinements, as well as increase quality of testware, improve cooperation, and enhance the quality of the test basis."
    },
    {
      "id": "q032",
      "chapterSection": "2.1.6",
      "questionText": "Why are retrospectives essential for continuous improvement?",
      "options": [
        "They produce formal documentation required by audits",
        "They replace other quality assurance activities",
        "They completely prevent the recurrence of past mistakes",
        "They ensure improvement actions are tracked and implemented"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Retrospectives are only effective for continuous improvement if their outcomes are followed up with implementation of recommended improvements."
    },
    {
      "id": "q033",
      "chapterSection": "2.1.6",
      "questionText": "When can retrospectives be organized, according to the syllabus?",
      "options": [
        "At the end of a project, at release milestones, at the end of an iteration, or whenever needed",
        "Only after a full project has been completed",
        "At fixed monthly intervals regardless of project events",
        "Only in response to serious production defects"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Retrospectives can take place at project end, iteration end, at release milestones, or when needed, depending on the SDLC model and organizational needs."
    },
    {
      "id": "q034",
      "chapterSection": "2.2.1",
      "questionText": "What is the primary objective of component integration testing?",
      "options": [
        "Testing each component in isolation",
        "Testing the interactions and interfaces between components",
        "Testing the overall system’s behavior and business flows",
        "Testing components using specific programming frameworks"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Component integration testing validates that components interact correctly and their interfaces work as intended. Component testing, by contrast, checks individual components in isolation."
    },
    {
      "id": "q035",
      "chapterSection": "2.2.1",
      "questionText": "Which test level is best suited for verifying end-to-end tasks and non-functional characteristics on a complete system?",
      "options": [
        "Component testing using unit frameworks",
        "Component integration testing with stubs and drivers",
        "System testing in a representative environment",
        "Acceptance testing by customer representatives"
      ],
      "correctAnswerIndex": 2,
      "explanation": "System testing evaluates end-to-end functionality and non-functional quality characteristics within a representative environment, covering the full system behavior."
    },
    {
      "id": "q036",
      "chapterSection": "2.2.1",
      "questionText": "What is the main difference between system testing and system integration testing?",
      "options": [
        "System testing evaluates complete system behavior, while system integration testing validates external interfaces with other systems",
        "System testing is performed only by developers, while system integration testing is performed by business users",
        "System testing requires white-box methods, while system integration testing requires black-box methods",
        "System testing always needs less documentation than system integration testing"
      ],
      "correctAnswerIndex": 0,
      "explanation": "System testing focuses on the overall system behavior and quality, while system integration testing targets interfaces and interactions with external systems or services."
    },
    {
      "id": "q037",
      "chapterSection": "2.2.1",
      "questionText": "Which statement best describes the purpose of acceptance testing?",
      "options": [
        "It ensures technical accuracy by being carried out only by the development team",
        "It primarily finds architectural defects before release",
        "It replaces the need for system testing in Agile teams",
        "It validates the system against business needs and confirms readiness for deployment"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Acceptance testing validates whether the system meets business needs and is ready for deployment. It is typically performed by users or customers and includes UAT, operational acceptance, and regulatory testing."
    },
    {
      "id": "q038",
      "chapterSection": "2.2.2",
      "questionText": "What is the main goal of functional testing?",
      "options": [
        "Measuring how the system performs under load and stress",
        "Evaluating completeness, correctness, and appropriateness of functions",
        "Ensuring code coverage is achieved at an acceptable level",
        "Checking alignment of the system with user expectations only"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Functional testing ensures that the system’s functions are complete, correct, and appropriate, verifying what the system does rather than how it does it."
    },
    {
      "id": "q039",
      "chapterSection": "2.2.2",
      "questionText": "Which option lists characteristics that are NOT considered non-functional according to ISO/IEC 25010?",
      "options": [
        "Performance efficiency and compatibility",
        "Security and maintainability",
        "Reliability and portability",
        "Functional completeness and correctness"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Functional completeness and correctness belong to functional quality, not non-functional. ISO/IEC 25010 defines non-functional qualities such as performance, compatibility, usability, reliability, security, maintainability, and portability."
    },
    {
      "id": "q040",
      "chapterSection": "2.2.2",
      "questionText": "Why can discovering non-functional defects late in the project cause major problems?",
      "options": [
        "They may require architectural or infrastructure changes that are costly to implement late",
        "They usually indicate that functional defects were overlooked earlier",
        "They only appear in production environments and are harder to reproduce",
        "They can be fixed only by delaying deployment until the next release"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Non-functional defects discovered late often require large-scale changes to architecture, design, or infrastructure. Addressing these issues late significantly increases cost and risk compared to finding them earlier."
    },
    {
      "id": "q041",
      "chapterSection": "2.2.2",
      "questionText": "What best describes the difference between black-box and white-box testing?",
      "options": [
        "Black-box testing focuses on external behavior, while white-box testing examines internal structure",
        "Black-box testing verifies performance, while white-box testing validates functionality",
        "Black-box testing is mostly manual, while white-box testing is mainly automated",
        "Black-box testing is carried out by testers, while white-box testing is carried out by developers"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Black-box testing is specification-based and focuses on external behavior against requirements, while white-box testing is structure-based and uses knowledge of the internal code, logic, or architecture."
    },
    {
      "id": "q042",
      "chapterSection": "2.2.3",
      "questionText": "What is the main goal of confirmation testing?",
      "options": [
        "To ensure system stability after a change",
        "To check that a reported defect has been corrected",
        "To verify that performance meets agreed benchmarks",
        "To confirm that new requirements have been implemented correctly"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Confirmation testing directly checks whether a reported defect has been fixed by re-running the relevant tests that previously failed."
    },
    {
      "id": "q043",
      "chapterSection": "2.2.3",
      "questionText": "What is the most effective way to reduce the scope of regression testing?",
      "options": [
        "Perform an impact analysis to identify affected areas",
        "Re-execute every test case from the start of the project",
        "Limit testing only to the component that was changed",
        "Only retest high-priority requirements to save time"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Impact analysis helps testers determine which areas could be affected by a change, making regression testing more focused and efficient."
    },
    {
      "id": "q044",
      "chapterSection": "2.2.3",
      "questionText": "Why is regression testing well-suited for automation?",
      "options": [
        "Regression testing requires little to no maintenance of test scripts",
        "Regression suites are run frequently and tend to grow larger over time",
        "Automated regression testing always finds more defects than manual testing",
        "Regression tests are easier to design compared to functional tests"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Regression test suites are run repeatedly and usually grow with each release. Automating them increases efficiency and reduces effort over time."
    },
    {
      "id": "q045",
      "chapterSection": "2.2.1",
      "questionText": "What characteristic is used to differentiate test levels and avoid overlap?",
      "options": [
        "The tools and techniques used in the testing process",
        "The specific test objects, objectives, basis, and responsibilities",
        "The programming languages and frameworks chosen",
        "The available project budget and assigned resources"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Test levels are distinguished by their test objects, objectives, basis, expected failures, and responsibilities. These distinctions prevent unnecessary overlap between levels."
    },
    {
      "id": "q046",
      "chapterSection": "2.2.1",
      "questionText": "Who usually performs component testing, and where is it done?",
      "options": [
        "Independent testers in a staging environment",
        "End users in acceptance environments",
        "Developers in their development environments",
        "System administrators in production-like environments"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Component (unit) testing is usually carried out by developers in their own development environments using tools like test harnesses or unit test frameworks."
    },
    {
      "id": "q047",
      "chapterSection": "2.2.2",
      "questionText": "When should non-functional testing ideally begin in the development lifecycle?",
      "options": [
        "As early as possible, during reviews and early test levels",
        "Only once functional testing is fully complete",
        "Exclusively during the system testing phase",
        "Only when user acceptance testing begins"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Non-functional testing should begin early, sometimes even during reviews or component testing, to catch critical issues before they become too expensive to fix."
    },
    {
      "id": "q048",
      "chapterSection": "2.2.2",
      "questionText": "How do many non-functional tests relate to functional tests?",
      "options": [
        "They replace functional tests entirely by covering broader quality characteristics",
        "They must always be designed independently from functional tests",
        "They extend functional tests by checking additional constraints such as performance or portability",
        "They only validate the outcomes of functional test cases"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Many non-functional tests build on functional tests, using the same functions but checking that they meet non-functional requirements such as performance or security."
    },
    {
      "id": "q049",
      "chapterSection": "2.2",
      "questionText": "What distinguishes test levels from test types?",
      "options": [
        "Test levels are automated processes, while test types are manual processes",
        "Test levels are performed by different teams, while test types use different tools",
        "Test levels are executed sequentially, while test types are always parallel",
        "Test levels are tied to stages of development, while test types focus on quality attributes"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Test levels align with development stages, while test types group activities based on quality characteristics like performance or security. Test types can be applied across multiple test levels."
    },
    {
      "id": "q050",
      "chapterSection": "2.2.3",
      "questionText": "In which situation might confirmation testing be simplified instead of fully re-executed?",
      "options": [
        "When the defect is in a high-risk, business-critical module",
        "When multiple integrations are involved across systems",
        "When resources such as time or budget are limited",
        "When formal certification or compliance approval is required"
      ],
      "correctAnswerIndex": 2,
      "explanation": "If time or budget is limited, confirmation testing might only re-execute the defect steps to verify the failure no longer occurs, instead of running all related tests."
    },
    {
      "id": "q051",
      "chapterSection": "2.2.3",
      "questionText": "Beyond the component where a change occurred, what else can regression testing affect?",
      "options": [
        "Only the changed component itself",
        "Primarily the user interface layer",
        "Only the data storage and database layers",
        "Other components in the system, connected systems, or even the environment"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Regression testing ensures changes do not cause unintended side effects, which may impact other system components, connected systems, or even the operating environment."
    },
    {
      "id": "q052",
      "chapterSection": "2.2.1",
      "questionText": "Which testing types fall under the main forms of acceptance testing?",
      "options": [
        "Unit testing, integration testing, and system testing",
        "User acceptance, operational acceptance, contractual/regulatory acceptance, alpha and beta testing",
        "Functional, security, and performance testing",
        "Black-box, white-box, and gray-box testing"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Acceptance testing includes user, operational, contractual/regulatory, and alpha/beta testing, focused on readiness for deployment and meeting business goals."
    },
    {
      "id": "q053",
      "chapterSection": "2.2",
      "questionText": "How do test levels typically relate in sequential versus iterative development models?",
      "options": [
        "Both sequential and iterative models follow identical test level relationships",
        "Iterative models generally require more levels of testing than sequential ones",
        "Sequential models exclude certain levels of testing that iterative models always include",
        "Sequential models use exit criteria of one level as entry criteria for the next, while iterative models may overlap levels"
      ],
      "correctAnswerIndex": 3,
      "explanation": "In sequential models, test levels flow linearly, with exit criteria feeding into entry criteria. Iterative models may blur these boundaries with overlap across activities."
    },
    {
      "id": "q054",
      "chapterSection": "2.3",
      "questionText": "According to ISO/IEC 14764, which categories describe software maintenance?",
      "options": [
        "Preventive, predictive, and reactive",
        "Corrective, adaptive to environmental changes, and performance/maintainability improvements",
        "Functional, non-functional, and structural",
        "Planned, unplanned, and emergency"
      ],
      "correctAnswerIndex": 1,
      "explanation": "ISO/IEC 14764 defines corrective, adaptive, and performance/maintainability improvements as the main categories of maintenance."
    },
    {
      "id": "q055",
      "chapterSection": "2.3",
      "questionText": "Why is impact analysis performed before implementing a change?",
      "options": [
        "To evaluate whether the change is worth the potential risks to other areas",
        "To estimate costs and delivery timelines",
        "To select the most suitable testing technique",
        "To determine which developers should be assigned to the change"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Impact analysis helps determine if the benefits of a change outweigh the risks and identifies possible side effects in other parts of the system."
    },
    {
      "id": "q056",
      "chapterSection": "2.3",
      "questionText": "When deploying a change to a live system, what are the two key aspects to test?",
      "options": [
        "Performance efficiency and security compliance",
        "User acceptance and regulatory approval",
        "Whether the change was successfully implemented and whether unchanged areas still work",
        "Correctness of functionality and all non-functional attributes"
      ],
      "correctAnswerIndex": 2,
      "explanation": "In production, testing focuses on confirming the change works as intended and checking for regressions in unchanged areas."
    },
    {
      "id": "q057",
      "chapterSection": "2.3",
      "questionText": "Which factor least influences the scope of maintenance testing?",
      "options": [
        "The degree of risk associated with the change",
        "The size of the existing system",
        "The size of the change itself",
        "The programming language used to build the system"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Risk level, system size, and change size influence maintenance testing scope. The programming language is not a primary factor."
    },
    {
      "id": "q058",
      "chapterSection": "2.3",
      "questionText": "What best describes hot fixes in software maintenance?",
      "options": [
        "Scheduled feature enhancements in upcoming releases",
        "Planned performance improvements delivered in cycles",
        "Unplanned urgent deployments to resolve critical problems",
        "Routine updates designed to improve maintainability"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Hot fixes are urgent, unplanned changes, typically delivered outside regular release cycles to quickly address issues."
    },
    {
      "id": "q059",
      "chapterSection": "2.3",
      "questionText": "When retiring an application, which type of testing may still be required?",
      "options": [
        "Performance testing to maximize final usage",
        "Security testing to prevent attacks during shutdown",
        "Verification of data archiving and retrieval processes",
        "Integration testing with future replacement systems"
      ],
      "correctAnswerIndex": 2,
      "explanation": "At system retirement, testing may include verifying that data archiving and retrieval functions work correctly to meet retention requirements."
    },
    {
      "id": "q060",
      "chapterSection": "2.3",
      "questionText": "During platform migration, which areas should be prioritized for testing?",
      "options": [
        "Core system features that remain unchanged",
        "User interface consistency across different devices",
        "Only network performance and bandwidth",
        "New environment setup, modified software, and data conversion if applicable"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Upgrades or migrations require tests associated with the new environment, changed software, and data conversion when applicable."
    },
    {
      "id": "q061",
      "chapterSection": "2.3",
      "questionText": "How does the size of a change typically impact maintenance testing?",
      "options": [
        "Change size only influences project cost, not testing",
        "Change size is one factor that helps define maintenance testing scope",
        "Smaller changes always require full regression testing",
        "Larger changes always reduce testing effort"
      ],
      "correctAnswerIndex": 1,
      "explanation": "Change size is one of the factors that determines the scope of maintenance testing, along with risk and system size."
    },
    {
      "id": "q062",
      "chapterSection": "2.3",
      "questionText": "When testing data archiving during system retirement, what is most important?",
      "options": [
        "Verifying that archived data is compressed to save space",
        "Confirming all archived data is permanently deleted",
        "Checking that archived data is transferred to cloud storage",
        "Ensuring archived data can be restored and accessed when needed"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Testing of restore and retrieval procedures ensures that archived data remains accessible during the retention period."
    },
    {
      "id": "q063",
      "chapterSection": "2.3",
      "questionText": "Which of the following is an example of adaptive maintenance?",
      "options": [
        "Fixing defects that cause incorrect results",
        "Optimizing queries to improve performance",
        "Updating the system to work with a new operating system",
        "Adding new user-requested features"
      ],
      "correctAnswerIndex": 2,
      "explanation": "Adaptive maintenance adjusts the system to changes in the environment, such as a new OS version."
    },
    {
      "id": "q064",
      "chapterSection": "2.3",
      "questionText": "Why is regression testing a key activity in maintenance testing?",
      "options": [
        "Most of the system remains unchanged and needs verification",
        "Regression tests are faster than other test types",
        "Maintenance changes usually affect the entire architecture",
        "Regression testing only validates documentation updates"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Regression testing ensures unchanged parts of the system are not negatively affected by maintenance changes."
    },
    {
      "id": "q065",
      "chapterSection": "2.3",
      "questionText": "What distinguishes planned enhancements from hot fixes?",
      "options": [
        "Planned enhancements are scheduled releases, hot fixes are unplanned urgent changes",
        "Hot fixes add functionality, enhancements only address security issues",
        "Enhancements always require more effort than hot fixes",
        "Hot fixes require no testing, enhancements do"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Planned enhancements follow a release cycle, while hot fixes are unplanned urgent deployments."
    },
    {
      "id": "q066",
      "chapterSection": "2.3",
      "questionText": "In which case is data conversion testing most likely required?",
      "options": [
        "When migrating data from another application into the system",
        "When applying a small bug fix in production",
        "During normal performance optimization activities",
        "When archiving old unused data"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Data conversion testing is needed when migrating data from another application into the maintained system."
    },
    {
      "id": "q067",
      "chapterSection": "2.3",
      "questionText": "How does the size of the existing system affect maintenance testing?",
      "options": [
        "Larger systems always require complete re-testing",
        "Smaller systems usually require more complex testing",
        "System size dictates which programming language should be used",
        "System size is a factor that helps determine testing scope"
      ],
      "correctAnswerIndex": 3,
      "explanation": "System size is one of the three main factors that determine the scope of maintenance testing."
    },
    {
      "id": "q068",
      "chapterSection": "2.3",
      "questionText": "Which type of maintenance improves performance or maintainability without adding features?",
      "options": [
        "Adaptive maintenance",
        "Corrective maintenance",
        "Preventive maintenance",
        "Perfective maintenance"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Perfective maintenance focuses on improving performance or maintainability without adding new functionality."
    },
    {
      "id": "q069",
      "chapterSection": "2.3",
      "questionText": "Why is testing important when long-term data retention is required after system retirement?",
      "options": [
        "To ensure archived data is preserved and retrievable",
        "To confirm data is compressed to minimum size",
        "To verify the data is deleted after a set period",
        "To check data can be encrypted with stronger algorithms"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Data archiving testing ensures that long-retention data is properly preserved and retrievable during the archiving period."
    },
    {
      "id": "q070",
      "chapterSection": "2.3",
      "questionText": "What is the primary goal when evaluating a change in production?",
      "options": [
        "Confirming the change improves overall system performance",
        "Checking the change follows coding standards",
        "Ensuring the change was implemented correctly and functions as intended",
        "Verifying the change reduced operating costs"
      ],
      "correctAnswerIndex": 2,
      "explanation": "The goal is to confirm the change was implemented correctly and works as intended in production."
    },
    {
      "id": "q071",
      "chapterSection": "2.3",
      "questionText": "Which factor increases the risk level of a maintenance change?",
      "options": [
        "The change requires new hardware resources",
        "The change is developed by an external vendor",
        "The change uses an updated programming framework",
        "The change impacts multiple interconnected components"
      ],
      "correctAnswerIndex": 3,
      "explanation": "Changes affecting multiple interconnected components have higher risk due to their potential widespread impact."
    },
    {
      "id": "q072",
      "chapterSection": "2.3",
      "questionText": "Why is environment-related testing important during platform upgrades?",
      "options": [
        "Software behavior may differ in the new environment",
        "Platform upgrades always improve system performance",
        "Environment changes affect only non-functional aspects",
        "Upgrades never require software reconfiguration"
      ],
      "correctAnswerIndex": 0,
      "explanation": "Software may behave differently in a new environment, so environment-related testing ensures compatibility and correct functioning."
    },
    {
      "id": "q073",
      "chapterSection": "2.3",
      "questionText": "How do the three main triggers for maintenance testing differ?",
      "options": [
        "Modifications affect code, upgrades affect environments, retirement focuses on data preservation",
        "They require different tools depending on project budget",
        "They always require the same test strategy but in different order",
        "They are handled by different types of project managers"
      ],
      "correctAnswerIndex": 0,
      "explanation": "The triggers differ: modifications affect code, upgrades affect environments, and retirement focuses on data archiving and preservation."
    }
  ]
}
